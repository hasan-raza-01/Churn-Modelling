# Churn Modelling: A Productionâ€‘Grade Customer Churn Prediction Platform

An endâ€‘toâ€‘end, MLOpsâ€‘driven pipeline for automated customer churn prediction. By combining PyTorchâ€‘powered neural networks, DVC versioning, MLflow experiment tracking, and an interactive Gradio interface, this system delivers accurate, traceable churn forecastsâ€”ready for production.

---

## Hereâ€™s a preview of the appâ€™s user interface:
![UI Screenshot](./screenshots/ui-preview.png)

---

## ğŸ“‚ Repository Structure

```
.
â”œâ”€â”€ .dvc/                          # DVC configuration & cache
â”œâ”€â”€ .github/workflows/             # CI/CD pipelines
â”œâ”€â”€ config/                        # Project-wide YAML config
â”œâ”€â”€ ml-app/                        # Dockerfile & settings for Gradio app
â”œâ”€â”€ mlflow-server/                 # Dockerfile & setup for MLflow server
â”œâ”€â”€ notebook/                      # Exploratory data analysis notebooks
â”œâ”€â”€ schema/                        # Saved schema definitions (YAML)
â”œâ”€â”€ src/
â”‚   â””â”€â”€ churn_modelling/           # Package source
â”‚       â”œâ”€â”€ configuration.py       # Config dataclasses
â”‚       â”œâ”€â”€ pipeline/              # Ingestion, validation, transformation, training, prediction
â”‚       â”œâ”€â”€ utils/                 # Model helpers, file I/O
â”‚       â”œâ”€â”€ logger.py              # Logging setup
â”‚       â””â”€â”€ exception.py           # Custom exceptions
â”œâ”€â”€ ETL.py                         # Orchestrates ingestion, DB & S3 push, schema save
â”œâ”€â”€ ProjectConfig.json             # Basic project metadata
â”œâ”€â”€ docker-compose.yml             # Multiâ€‘service orchestration
â”œâ”€â”€ dvc.yaml / dvc.lock            # Pipeline stage definitions & lock file
â”œâ”€â”€ params.json                    # Pipeline hyperparameters
â”œâ”€â”€ requirements.txt               # Python dependencies
â””â”€â”€ setup.py                       # Package installer
```

---

## ğŸ”§ Core Workflow

1. **Data Ingestion**
   Uses DVC to pull raw customer data (CSV) from remote storage, and runs `stage_01_data_ingestion.py` to persist cleaned datasets.

2. **Data Validation**
   Validates schema and missing values via `stage_02_data_validation.py`, ensuring data quality before transformation.

3. **Data Transformation**
   Encodes, scales, and engineers features in `stage_03_data_transformation.py`, outputting modelâ€‘ready training and test sets.

4. **Model Training**
   Trains and tunes a PyTorch + Skorch ANN through `stage_04_model_trainer.py`, logs metrics/artifacts to MLflow, and saves best weights.

5. **Realâ€‘Time Gradio Interface**

   * **Training Trigger**: â€œRun Training Pipelineâ€ button invokes the full DVCâ†’MLflow pipeline.
   * **Churn Prediction**: Live inputs (customer age, balance, tenure, etc.) feed into the saved model via `predict_churn()`.
   * **Deployment**: Exposed at `http://localhost:7860` by default, with a clean, userâ€‘friendly UI.

---

## âœ… Key Capabilities

* **Featureâ€‘Grounded Predictions**
  Answers grounded in real customer featuresâ€”credit score, geography, balance, usage patterns.
* **Full MLOps Stack**

  * **DVC** for data & artifact versioning
  * **MLflow** for experiment tracking & artifact storage in S3
  * **Structured Logs & Custom Exceptions** for robust pipeline observability
* **Interactive UI**
  Gradio app for nonâ€‘technical stakeholders to train models and predict churn in seconds.
* **Modular & Extensible**
  Clear separation of ingestion, validation, transformation, training, and inference; swap out model architectures or data sources with minimal changes.
* **Containerized Deployment**
  Dockerfiles for both the MLflow server and the Gradio app; orchestrated via Docker Compose for seamless local or cloud deployment.

---

## ğŸš€ Deployment & CI/CD

* **GitHub Actions**
  Automates DVC pulls, linting, testing, and Docker image builds on every commit (`.github/workflows/`).
* **Docker Compose**

  * **mlflow-server**: Builds from `mlflow-server/`, exposes port 5000, persists MLflow runs to a Docker volume.
  * **ml-app**: Builds from `ml-app/Dockerfile`, exposes port 7860, reads secrets from your `.env`, depends on `mlflow-server`.
* **Environmentâ€‘Driven Configuration**
  Store credentials and endpoints in a `.env` (referenced by `docker-compose.yml`):

  ```
  S3_BUCKET=""
  S3_BUCKET_OBJECT=""
  S3_BUCKET_PREDICTION_OBJECT=""
  S3_BUCKET_MLFLOW_DIR=""
  MLFLOW_S3_ENDPOINT_URL="https://s3.amazonaws.com"
  MLFLOW_TRACKING_URI="http://mlflow-server:5000"
  ```

---

## ğŸƒ Running Locally

1. **Clone & Enter**

   ```bash
   git clone https://github.com/hasan-raza-01/Churn-Modelling.git
   cd Churn-Modelling
   ```

2. **Install Dependencies**
  - ***Upgrade/Install pip and uv***
    ```bash
    pip install --upgrade pip uv
    ```
  - ***create virtual environment through uv***
    ```bash
    uv venv .venv --python 3.12
    ```
  - ***activate the environment***
    - ***Command Prompt / PowerShell***
      ```bash
      .venv\scripts\activate
      ```
    - ***Git Bash***
      ```
      source .venv/scripts/activate
      ```
  - ***Install required packages***
    ```
    uv pip install -e .
    ```

3. **Environment Variable MLFLOW_TRACKING_URI**
  - ***local machine*** 
  ```
  http://localhost:5000
  ```
  - ***dockerize/deployment*** 
  ```
  http://mlflow-server:5000
  ```

4. **env file path [env_file] in docker-compose.yml** 
  - ***local machine dockerization*** 
  ```
  .env
  ```
  - ***Aws Cloud deployment*** 
  ```
  /home/ubuntu/.env
  ```

5. **Run app**
   #### **Run ETL[Extract Transform Load] Pipeline**
   #### ***Note: Change variable named 'data_path' inside section [__name__ == "__main__"] of ETL.py with path/of/data/inside/your/local/system***
    ```
    uv run ETL.py
    ```
  - **Docker**
    - ***build and run images***
    ```bash
    docker-compose up --build
    ```

  - **Manuall**
    - **MLflow Server Launch**
      #### ***Before running the app, start the tracking server:***
      #### ***Note: change 'your-bucket' & 'path' from s3 bucket and path/of/.db/file respectively.***
      ```bash
      mlflow server \
        --backend-store-uri sqlite:///mlruns/mlflow.db \
        --default-artifact-root s3://<your-bucket>/<path>/ \
        --host 0.0.0.0 \
        --port 5000
      ```
    - **Run application**
      #### ***Note: On first run It will take time for creation of artifacts***
      ```
      uv run app.py
      ```

6. **Visit the UI**
   Open your browser to `http://localhost:7860` to train the model or predict churn in real time.
