{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e66cb29b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b638da05",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4c93dc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9c8933f",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f2c4419",
   "metadata": {},
   "source": [
    "# constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "658fd49b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from churn_modelling.utils import load_yaml\n",
    "from dataclasses import dataclass \n",
    "\n",
    "\n",
    "CONFIG = load_yaml('config/config.yaml')\n",
    "\n",
    "@dataclass \n",
    "class DataTransformationConstants:\n",
    "    ROOT_DIR = CONFIG.ROOT_DIR\n",
    "    DATA_ROOT_DIR = CONFIG.DATA.ROOT_DIR\n",
    "    TRANSFORMATION_ROOT_DIR = CONFIG.DATA.TRANSFORMATION.ROOT_DIR\n",
    "    TRAIN_DATA = CONFIG.DATA.TRANSFORMATION.TRAIN_DATA\n",
    "    TEST_DATA = CONFIG.DATA.TRANSFORMATION.TEST_DATA\n",
    "    FEATURES = CONFIG.DATA.TRANSFORMATION.FEATURES\n",
    "    PREPROCESSOR = CONFIG.DATA.TRANSFORMATION.PREPROCESSOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "790cf82f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'ROOT_DIR:{DataTransformationConstants.ROOT_DIR}')\n",
    "print(f'DATA_ROOT_DIR:{DataTransformationConstants.DATA_ROOT_DIR}')\n",
    "print(f'TRANSFORMATION_ROOT_DIR:{DataTransformationConstants.TRANSFORMATION_ROOT_DIR}')\n",
    "print(f'TRAIN_DATA:{DataTransformationConstants.TRAIN_DATA}')\n",
    "print(f'TEST_DATA:{DataTransformationConstants.TEST_DATA}')\n",
    "print(f'FEATURES:{DataTransformationConstants.FEATURES}')\n",
    "print(f'PREPROCESSOR:{DataTransformationConstants.PREPROCESSOR}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52ef03e3",
   "metadata": {},
   "source": [
    "# entity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd385ba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass \n",
    "from typing import ClassVar \n",
    "from pathlib import Path \n",
    "\n",
    "\n",
    "@dataclass \n",
    "class DataTransformation:\n",
    "    ROOT_DIR_PATH:ClassVar[Path]\n",
    "    DATA_ROOT_DIR_PATH:ClassVar[Path]\n",
    "    TRANSFORMATION_ROOT_DIR_PATH:ClassVar[Path]\n",
    "    TRAIN_DATA_FILE_PATH:ClassVar[Path]\n",
    "    TEST_DATA_FILE_PATH:ClassVar[Path]\n",
    "    FEATURES_FILE_PATH:ClassVar[Path]\n",
    "    PREPROCESSOR_FILE_PATH:ClassVar[Path]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44e16e75",
   "metadata": {},
   "source": [
    "# configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a48b02f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass \n",
    "from pathlib import Path \n",
    "import os \n",
    "\n",
    "\n",
    "@dataclass \n",
    "class DataTransformationConfig:\n",
    "    ROOT_DIR_PATH = Path(DataTransformationConstants.ROOT_DIR)\n",
    "    DATA_ROOT_DIR_PATH = Path(os.path.join(ROOT_DIR_PATH, DataTransformationConstants.DATA_ROOT_DIR))\n",
    "    TRANSFORMATION_ROOT_DIR_PATH = Path(os.path.join(DATA_ROOT_DIR_PATH, DataTransformationConstants.TRANSFORMATION_ROOT_DIR))\n",
    "    TRAIN_DATA_FILE_PATH = Path(os.path.join(TRANSFORMATION_ROOT_DIR_PATH, DataTransformationConstants.TRAIN_DATA))\n",
    "    TEST_DATA_FILE_PATH = Path(os.path.join(TRANSFORMATION_ROOT_DIR_PATH, DataTransformationConstants.TEST_DATA))\n",
    "    FEATURES_FILE_PATH = Path(os.path.join(TRANSFORMATION_ROOT_DIR_PATH, DataTransformationConstants.FEATURES))\n",
    "    PREPROCESSOR_FILE_PATH = Path(os.path.join(TRANSFORMATION_ROOT_DIR_PATH, DataTransformationConstants.PREPROCESSOR))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1373acf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'ROOT_DIR_PATH:{DataTransformationConfig.ROOT_DIR_PATH}')\n",
    "print(f'DATA_ROOT_DIR_PATH:{DataTransformationConfig.DATA_ROOT_DIR_PATH}')\n",
    "print(f'TRANSFORMATION_ROOT_DIR_PATH:{DataTransformationConfig.TRANSFORMATION_ROOT_DIR_PATH}')\n",
    "print(f'TRAIN_DATA_FILE_PATH:{DataTransformationConfig.TRAIN_DATA_FILE_PATH}')\n",
    "print(f'TEST_DATA_FILE_PATH:{DataTransformationConfig.TEST_DATA_FILE_PATH}')\n",
    "print(f'FEATURES_FILE_PATH:{DataTransformationConfig.FEATURES_FILE_PATH}')\n",
    "print(f'PREPROCESSOR_FILE_PATH:{DataTransformationConfig.PREPROCESSOR_FILE_PATH}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d78dd18",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'ROOT_DIR_PATH:{type(DataTransformationConfig.ROOT_DIR_PATH)}')\n",
    "print(f'DATA_ROOT_DIR_PATH:{type(DataTransformationConfig.DATA_ROOT_DIR_PATH)}')\n",
    "print(f'TRANSFORMATION_ROOT_DIR_PATH:{type(DataTransformationConfig.TRANSFORMATION_ROOT_DIR_PATH)}')\n",
    "print(f'TRAIN_DATA_FILE_PATH:{type(DataTransformationConfig.TRAIN_DATA_FILE_PATH)}')\n",
    "print(f'TEST_DATA_FILE_PATH:{type(DataTransformationConfig.TEST_DATA_FILE_PATH)}')\n",
    "print(f'FEATURES_FILE_PATH:{type(DataTransformationConfig.FEATURES_FILE_PATH)}')\n",
    "print(f'PREPROCESSOR_FILE_PATH:{type(DataTransformationConfig.PREPROCESSOR_FILE_PATH)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eea1f3e8",
   "metadata": {},
   "source": [
    "# components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6260de2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from churn_modelling.utils import dump_json, load_json, create_dirs, save_pickle\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler \n",
    "from churn_modelling.exception import CustomException \n",
    "from churn_modelling.entity import DataValidation \n",
    "from sklearn.compose import ColumnTransformer\n",
    "from churn_modelling.logger import logging\n",
    "from sklearn.pipeline import Pipeline \n",
    "from sklearn.utils import resample \n",
    "from dataclasses import dataclass \n",
    "import pandas as pd\n",
    "import sys \n",
    "\n",
    "\n",
    "\n",
    "@dataclass \n",
    "class DataTransformationComponents:\n",
    "    data_validation_config:DataValidation\n",
    "    data_transformation_config: DataTransformation \n",
    "\n",
    "    def load_data(self):\n",
    "        try:\n",
    "            logging.info('In load_data')\n",
    "\n",
    "            # load validation report\n",
    "            report_path = self.data_validation_config.REPORT_FILE_FILE_PATH \n",
    "            report = load_json(report_path)\n",
    "            logging.info(f'loaded report from {{{report_path}}}')\n",
    "\n",
    "            # load the data if validation status of data is true \n",
    "            for key, value in report.items():\n",
    "                if key.strip().lower() == 'train':\n",
    "                    if value['status']:\n",
    "                        train_data_path = self.data_validation_config.VALID_TRAIN_DATA_FILE_PATH\n",
    "                        self.train_data = pd.read_csv(train_data_path)\n",
    "                        logging.info(f'train data loaded from {{{train_data_path}}}')\n",
    "                if key.strip().lower() == 'test':\n",
    "                    if value['status']:\n",
    "                        test_data_path = self.data_validation_config.VALID_TEST_DATA_FILE_PATH\n",
    "                        self.test_data = pd.read_csv(test_data_path)\n",
    "                        logging.info(f'test data loaded from {{{test_data_path}}}') \n",
    "\n",
    "            logging.info('Out load_data') \n",
    "        except Exception as e:\n",
    "            logging.exception(e)\n",
    "            raise CustomException(e, sys)\n",
    "\n",
    "    def transform_data(self):\n",
    "        try:\n",
    "            logging.info('In transform_data')\n",
    "\n",
    "            # remove duplidates\n",
    "            self.train_data.drop_duplicates(inplace=True)\n",
    "            self.test_data.drop_duplicates(inplace=True)\n",
    "            logging.info(f'droped duplicates')\n",
    "\n",
    "            # remove unnecessory columns \n",
    "            unnecessory_features = [\"RowNumber\", \"CustomerId\", \"Surname\"] \n",
    "            self.train_data.drop(unnecessory_features, axis=1, inplace=True)\n",
    "            self.test_data.drop(unnecessory_features, axis=1, inplace=True)\n",
    "            logging.info(f'features after drop unnecessory columns\\ntrain:{self.train_data.columns}\\ntest:{self.test_data.columns}')\n",
    "\n",
    "            # target column \n",
    "            self.target = 'Exited'\n",
    "            logging.info(f'target column {{{self.target}}}')\n",
    "\n",
    "            # concatenate both train and test \n",
    "            data = pd.concat([self.train_data, self.test_data], axis=0)\n",
    "            logging.info(f'concatenated both train and test, shape[before:[{self.train_data.shape}, {self.test_data.shape}], after:{data.shape}]')\n",
    "\n",
    "            # distinguish numerical and categorical features\n",
    "            numerical_features = [feature for feature in data.columns if data[feature].dtype != \"O\" and len(data[feature].unique()) > 15 and feature != self.target]\n",
    "            categorical_features = [feature for feature in data.columns if feature not in numerical_features and feature != self.target]\n",
    "            logging.info(f'numerical features:{numerical_features}, count:{len(numerical_features)}\\ncategorical features:{categorical_features}, count:{len(categorical_features)}')\n",
    "\n",
    "            # handling of null values\n",
    "            # numerical features\n",
    "            # train data \n",
    "            for feature in numerical_features:\n",
    "                self.train_data.loc[:, feature] = self.train_data[feature].fillna(data[feature].mean())\n",
    "            logging.info('null values handling of numerical features for train data completed')\n",
    "            # test data \n",
    "            for feature in numerical_features:\n",
    "                self.test_data.loc[:, feature] = self.test_data[feature].fillna(data[feature].mean())\n",
    "            logging.info('null values handling of numerical features for test data completed')\n",
    "\n",
    "            # categorical features\n",
    "            # train data \n",
    "            for feature in categorical_features:\n",
    "                self.train_data.loc[:, feature] = self.train_data[feature].fillna(data[feature].mode()[0])\n",
    "            logging.info('null values handling of categorical features for train data completed')\n",
    "            # test data \n",
    "            for feature in categorical_features:\n",
    "                self.test_data.loc[:, feature] = self.test_data[feature].fillna(data[feature].mode()[0])\n",
    "            logging.info('null values handling of categorical features for test data completed')\n",
    "\n",
    "            # get categorical feature with type object to transform into numeric \n",
    "            categorical_features = [feature for feature in data.columns if data[feature].dtype == 'O' and feature not in numerical_features]\n",
    "            logging.info(f'categorical values which needs to be transformed from object to numeric {{{categorical_features}}}')\n",
    "\n",
    "            # pipelines\n",
    "            numerical_pipeline = Pipeline([\n",
    "                ('scaler', StandardScaler())\n",
    "            ])\n",
    "            categorical_pipeline = Pipeline([\n",
    "                ('encoder', OneHotEncoder())\n",
    "            ])\n",
    "            logging.info('pipelines created for both numerical and categorical features')\n",
    "\n",
    "            # final preprocessor \n",
    "            self.preprocessor = ColumnTransformer([\n",
    "                ('numerical_pipeline', numerical_pipeline, numerical_features),\n",
    "                ('categorical_pipeline', categorical_pipeline, categorical_features)\n",
    "            ], remainder='passthrough', n_jobs=-1, verbose=True, verbose_feature_names_out=True)\n",
    "            logging.info('column transformer initialized')\n",
    "\n",
    "            # transform data \n",
    "            # train data \n",
    "            transformed_train_data = self.preprocessor.fit_transform(self.train_data)\n",
    "            logging.info('successfully transformed train data')\n",
    "            # test data \n",
    "            transformed_test_data = self.preprocessor.fit_transform(self.test_data)\n",
    "            logging.info('successfully transformed test data')\n",
    "\n",
    "            # create data frame with transformed data \n",
    "            self.columns = [name.split('__')[1] for name in self.preprocessor.get_feature_names_out()]\n",
    "            logging.info(f'columns after transformation {self.columns}')\n",
    "            # train data \n",
    "            self.transformed_train_data = pd.DataFrame(transformed_train_data, columns=self.columns)\n",
    "            # test data \n",
    "            self.transformed_test_data = pd.DataFrame(transformed_test_data, columns=self.columns)\n",
    "            logging.info('converted transformed train and test data into dataframes')\n",
    "\n",
    "            # handle imbalnced training dataset\n",
    "            # get majority category\n",
    "            majority_class = {v:k for k, v in self.transformed_train_data[self.target].value_counts().to_dict().items()}[max({v:k for k, v in self.transformed_train_data[self.target].value_counts().to_dict().items()})]\n",
    "            minurity_class = {v:k for k, v in self.transformed_train_data[self.target].value_counts().to_dict().items()}[min({v:k for k, v in self.transformed_train_data[self.target].value_counts().to_dict().items()})]\n",
    "            logging.info(f'majority and minurity class of target column {{{majority_class}, {minurity_class}}}')\n",
    "            # majority_class and minurity_class data \n",
    "            train_data_majority_class = self.transformed_train_data[self.transformed_train_data[self.target]==majority_class]\n",
    "            train_data_minurity_class = self.transformed_train_data[self.transformed_train_data[self.target]==minurity_class]\n",
    "            logging.info(f'majority class of train data shape:{train_data_majority_class.shape}, minurity class of train data shape:{train_data_minurity_class.shape}')\n",
    "\n",
    "            # resampled train data of minurity class \n",
    "            train_data_minurity_class_resampled = resample(train_data_minurity_class, replace=True, n_samples=len(train_data_majority_class), random_state=42)\n",
    "            logging.info(f'shape after Upsampling; majority class of train data shape:{train_data_majority_class.shape}, minurity class of train data shape::{train_data_minurity_class_resampled.shape}')\n",
    "\n",
    "            # concatenate train data of majority and minurity class after resampling  \n",
    "            self.transformed_train_data_resampled = pd.concat([train_data_majority_class, train_data_minurity_class_resampled], axis=0)\n",
    "\n",
    "            logging.info('Out transform_data') \n",
    "        except Exception as e:\n",
    "            logging.exception(e)\n",
    "            raise CustomException(e, sys)\n",
    "\n",
    "    def save_outputs(self):\n",
    "        try:\n",
    "            logging.info('In save_outputs')\n",
    "\n",
    "            # train data \n",
    "            train_data_path = self.data_transformation_config.TRAIN_DATA_FILE_PATH\n",
    "            self.transformed_train_data_resampled.to_csv(train_data_path, index=False)\n",
    "            logging.info(f'saved train data at {{{train_data_path}}}')\n",
    "\n",
    "            # test data \n",
    "            test_data_path = self.data_transformation_config.TEST_DATA_FILE_PATH\n",
    "            self.transformed_test_data.to_csv(test_data_path, index=False)\n",
    "            logging.info(f'saved test data at {{{test_data_path}}}')\n",
    "\n",
    "            # feature names\n",
    "            feature_names_path = self.data_transformation_config.FEATURES_FILE_PATH\n",
    "            dump_json({'columns':self.columns}, feature_names_path)\n",
    "            logging.info(f'feature names saved at {{{feature_names_path}}}')\n",
    "\n",
    "            # preprocessor \n",
    "            preprocessor_path = self.data_transformation_config.PREPROCESSOR_FILE_PATH\n",
    "            save_pickle(preprocessor_path, self.preprocessor)\n",
    "            logging.info(f'preprocessor saved at {{{preprocessor_path}}}')\n",
    "\n",
    "            logging.info('Out save_outputs') \n",
    "        except Exception as e:\n",
    "            logging.exception(e)\n",
    "            raise CustomException(e, sys)\n",
    "        \n",
    "    def main(self):\n",
    "        # create required directoies \n",
    "        create_dirs(self.data_transformation_config.ROOT_DIR_PATH) \n",
    "        create_dirs(self.data_transformation_config.DATA_ROOT_DIR_PATH) \n",
    "        create_dirs(self.data_transformation_config.TRANSFORMATION_ROOT_DIR_PATH)\n",
    "\n",
    "        self.load_data()\n",
    "        self.transform_data()\n",
    "        self.save_outputs()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce487b34",
   "metadata": {},
   "source": [
    "# pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18b47e1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from churn_modelling.configuration import DataValidationConfig\n",
    "from dataclasses import dataclass \n",
    "\n",
    "\n",
    "@dataclass \n",
    "class DataTransformationPipeline:\n",
    "    def run(self):\n",
    "        obj = DataTransformationComponents(DataValidationConfig, DataTransformationConfig)\n",
    "        obj.main()\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    data_transformation_pipeline = DataTransformationPipeline()\n",
    "    data_transformation_pipeline.run()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9365c593",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Churn_Modelling",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
