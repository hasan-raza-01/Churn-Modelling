{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2200e24",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e873a42",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7541f02e",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "569fcaa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc9d07b0",
   "metadata": {},
   "source": [
    "# constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8b9e209",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass \n",
    "from churn_modelling.utils import load_yaml \n",
    "\n",
    "\n",
    "CONFIG = load_yaml(\"config/config.yaml\")\n",
    "\n",
    "@dataclass \n",
    "class ModelTrainerConstants:\n",
    "    ROOT_DIR = CONFIG.ROOT_DIR\n",
    "    MODEL_ROOT_DIR = CONFIG.MODEL.ROOT_DIR\n",
    "    TRAINING_ROOT_DIR = CONFIG.MODEL.TRAINING.ROOT_DIR\n",
    "    SCORES_FILE = CONFIG.MODEL.TRAINING.SCORES_FILE\n",
    "    BEST_PARAMS_FILE = CONFIG.MODEL.TRAINING.BEST_PARAMS_FILE\n",
    "    ESTIMATOR_FILE = CONFIG.MODEL.TRAINING.ESTIMATOR_FILE\n",
    "    PARAMS_FILE_PATH = \"params.json\"\n",
    "    TARGET = \"Exited\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f009214",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"ROOT_DIR:{ModelTrainerConstants.ROOT_DIR}\")\n",
    "print(f\"MODEL_ROOT_DIR:{ModelTrainerConstants.MODEL_ROOT_DIR}\")\n",
    "print(f\"TRAINING_ROOT_DIR:{ModelTrainerConstants.TRAINING_ROOT_DIR}\")\n",
    "print(f\"SCORES_FILE:{ModelTrainerConstants.SCORES_FILE}\")\n",
    "print(f\"BEST_PARAMS_FILE:{ModelTrainerConstants.BEST_PARAMS_FILE}\")\n",
    "print(f\"ESTIMATOR_FILE:{ModelTrainerConstants.ESTIMATOR_FILE}\")\n",
    "print(f\"PARAMS_FILE_PATH:{ModelTrainerConstants.PARAMS_FILE_PATH}\")\n",
    "print(f\"TARGET:{ModelTrainerConstants.TARGET}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "736d51dc",
   "metadata": {},
   "source": [
    "# entity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d21287b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass \n",
    "from typing import ClassVar \n",
    "from pathlib import Path \n",
    "\n",
    "\n",
    "\n",
    "@dataclass \n",
    "class ModelTrainer:\n",
    "    ROOT_DIR_PATH:ClassVar[Path]\n",
    "    MODEL_ROOT_DIR_PATH:ClassVar[Path]\n",
    "    TRAINING_ROOT_DIR_PATH:ClassVar[Path]\n",
    "    SCORES_FILE_PATH:ClassVar[Path]\n",
    "    BEST_PARAMS_FILE_PATH:ClassVar[Path]\n",
    "    ESTIMATOR_FILE_PATH:ClassVar[Path]\n",
    "    PARAMS_FILE_PATH:ClassVar[Path]\n",
    "    TARGET:str "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18671978",
   "metadata": {},
   "source": [
    "# configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36a2eb7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass \n",
    "from pathlib import Path \n",
    "import os \n",
    "\n",
    "\n",
    "\n",
    "@dataclass \n",
    "class ModelTrainerConfig:\n",
    "    ROOT_DIR_PATH = Path(ModelTrainerConstants.ROOT_DIR)\n",
    "    MODEL_ROOT_DIR_PATH = Path(os.path.join(ROOT_DIR_PATH, ModelTrainerConstants.MODEL_ROOT_DIR))\n",
    "    TRAINING_ROOT_DIR_PATH = Path(os.path.join(MODEL_ROOT_DIR_PATH, ModelTrainerConstants.TRAINING_ROOT_DIR))\n",
    "    SCORES_FILE_PATH = Path(os.path.join(TRAINING_ROOT_DIR_PATH, ModelTrainerConstants.SCORES_FILE))\n",
    "    BEST_PARAMS_FILE_PATH = Path(os.path.join(TRAINING_ROOT_DIR_PATH, ModelTrainerConstants.BEST_PARAMS_FILE))\n",
    "    ESTIMATOR_FILE_PATH = Path(os.path.join(TRAINING_ROOT_DIR_PATH, ModelTrainerConstants.ESTIMATOR_FILE))\n",
    "    PARAMS_FILE_PATH = Path(ModelTrainerConstants.PARAMS_FILE_PATH)\n",
    "    TARGET = ModelTrainerConstants.TARGET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85593a51",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"ROOT_DIR_PATH:{ModelTrainerConfig.ROOT_DIR_PATH}\")\n",
    "print(f\"MODEL_ROOT_DIR_PATH:{ModelTrainerConfig.MODEL_ROOT_DIR_PATH}\")\n",
    "print(f\"TRAINING_ROOT_DIR_PATH:{ModelTrainerConfig.TRAINING_ROOT_DIR_PATH}\")\n",
    "print(f\"SCORES_FILE_PATH:{ModelTrainerConfig.SCORES_FILE_PATH}\")\n",
    "print(f\"BEST_PARAMS_FILE_PATH:{ModelTrainerConfig.BEST_PARAMS_FILE_PATH}\")\n",
    "print(f\"ESTIMATOR_FILE_PATH:{ModelTrainerConfig.ESTIMATOR_FILE_PATH}\")\n",
    "print(f\"PARAMS_FILE_PATH:{ModelTrainerConfig.PARAMS_FILE_PATH}\")\n",
    "print(f\"TARGET:{ModelTrainerConfig.TARGET}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b563a19",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"ROOT_DIR_PATH:{type(ModelTrainerConfig.ROOT_DIR_PATH)}\")\n",
    "print(f\"MODEL_ROOT_DIR_PATH:{type(ModelTrainerConfig.MODEL_ROOT_DIR_PATH)}\")\n",
    "print(f\"TRAINING_ROOT_DIR_PATH:{type(ModelTrainerConfig.TRAINING_ROOT_DIR_PATH)}\")\n",
    "print(f\"SCORES_FILE_PATH:{type(ModelTrainerConfig.SCORES_FILE_PATH)}\")\n",
    "print(f\"BEST_PARAMS_FILE_PATH:{type(ModelTrainerConfig.BEST_PARAMS_FILE_PATH)}\")\n",
    "print(f\"ESTIMATOR_FILE_PATH:{type(ModelTrainerConfig.ESTIMATOR_FILE_PATH)}\")\n",
    "print(f\"PARAMS_FILE_PATH:{type(ModelTrainerConfig.PARAMS_FILE_PATH)}\")\n",
    "print(f\"TARGET:{type(ModelTrainerConfig.TARGET)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91edb939",
   "metadata": {},
   "source": [
    "# components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e86c3089",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from dataclasses import dataclass \n",
    "from skorch import NeuralNetClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from churn_modelling.entity import DataTransformation\n",
    "from churn_modelling.exception import CustomException \n",
    "from churn_modelling.logger import logging \n",
    "from sklearn.metrics import accuracy_score\n",
    "from churn_modelling.utils import load_json, create_dirs, dump_json, save_pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys, mlflow, os\n",
    "\n",
    "\n",
    "# set mlflow tracking uri\n",
    "mlflow.set_tracking_uri(os.getenv(\"MLFLOW_TRACKING_URI\"))\n",
    "\n",
    "# Define a custom neural network that supports a variable number of hidden layers.\n",
    "class ClassifierModule(nn.Module):\n",
    "    \"\"\"\n",
    "    A flexible neural network for a classification task that supports a tunable number\n",
    "    of hidden layers. The network architecture is defined as:\n",
    "    \n",
    "    Input -> [Hidden Layer 1 -> ReLU -> Dropout] -> ... -> [Hidden Layer N -> ReLU -> Dropout] -> Output Layer\n",
    "    \n",
    "    Attributes:\n",
    "      input_dim (int): Number of input features.\n",
    "      num_hidden_layers (int): Number of hidden layers in the network.\n",
    "      hidden_units (int): Number of neurons in each hidden layer.\n",
    "      dropout (float): Dropout probability applied after each hidden layer activation.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim=20, num_hidden_layers=1, hidden_units=50, dropout=0.5):\n",
    "        # Initialize the parent nn.Module class.\n",
    "        super().__init__()\n",
    "        \n",
    "        # Create a list to hold our hidden layers. We'll use the ModuleList container so that the layers\n",
    "        # are registered as submodules (required for proper parameter tracking during training).\n",
    "        hidden_layers = []\n",
    "        \n",
    "        # Add the first hidden layer: From input_dim to hidden_units.\n",
    "        hidden_layers.append(nn.Linear(input_dim, hidden_units))\n",
    "        \n",
    "        # Add additional hidden layers (if any) where each receives hidden_units as input and outputs hidden_units.\n",
    "        # We subtract one because the first layer is already added.\n",
    "        for _ in range(num_hidden_layers - 1):\n",
    "            hidden_layers.append(nn.Linear(hidden_units, hidden_units))\n",
    "        \n",
    "        # Save the list of hidden layers in a ModuleList so that it is properly managed.\n",
    "        self.hidden_layers = nn.ModuleList(hidden_layers)\n",
    "        \n",
    "        # Define a Dropout layer applied after each hidden layer activation.\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        # The final output layer maps the last hidden layer's output to the number of classes.\n",
    "        # Here, 2 is used for binary classification.\n",
    "        self.output_layer = nn.Linear(hidden_units, 2)\n",
    "    \n",
    "    def forward(self, *args, **kwargs):\n",
    "        \"\"\"\n",
    "        Defines the forward pass of the network.\n",
    "        \n",
    "        Arguments:\n",
    "          x (Tensor): Input tensor of shape (batch_size, input_dim)\n",
    "          \n",
    "        Returns:\n",
    "          Tensor: Logits output from the network.\n",
    "        \"\"\"\n",
    "        # If keyword arguments are provided, assume they are features and extract their values.\n",
    "        # This will convert the keys (e.g., 'CreditScore', â€¦) into a tensor.\n",
    "        if kwargs:\n",
    "            # Assuming all columns are numeric and should be concatenated along the feature axis.\n",
    "            x = torch.tensor([list(sample) for sample in zip(*kwargs.values())])\n",
    "        else:\n",
    "            x = args[0]  # usual case if input is a tensor\n",
    "\n",
    "        # Pass the input through each hidden layer block.\n",
    "        for layer in self.hidden_layers:\n",
    "            x = layer(x)       # Apply the linear transformation.\n",
    "            x = F.relu(x)      # Pass through ReLU activation to introduce non-linearity.\n",
    "            x = self.dropout(x)  # Apply dropout to reduce overfitting.\n",
    "        \n",
    "        # Pass the result from the last hidden layer block into the output layer.\n",
    "        x = self.output_layer(x)\n",
    "        # Note: We do not use softmax here; nn.CrossEntropyLoss expects raw logits.\n",
    "        return x\n",
    "\n",
    "@dataclass \n",
    "class ModelTrainerComponents:\n",
    "    data_transformation_config: DataTransformation\n",
    "    model_trainer_config:ModelTrainer\n",
    "\n",
    "    def load_data(self):\n",
    "        try:\n",
    "            logging.info(\"In load_data\")\n",
    "\n",
    "            # load transformed data \n",
    "            # train data \n",
    "            train_data_path = self.data_transformation_config.TRAIN_DATA_FILE_PATH\n",
    "            train_data = pd.read_csv(train_data_path)\n",
    "            logging.info(f\"train data loaded from {{{train_data_path}}}\")\n",
    "            # test data \n",
    "            test_data_path = self.data_transformation_config.TEST_DATA_FILE_PATH\n",
    "            test_data = pd.read_csv(test_data_path)\n",
    "            logging.info(f\"test data loaded from {{{test_data_path}}}\")\n",
    "\n",
    "            target = self.model_trainer_config.TARGET\n",
    "            self.X_train, self.y_train, self.X_test, self.y_test = train_data.drop(target, axis=1), train_data[target], test_data.drop(target, axis=1), test_data[target]\n",
    "            logging.info(f'X_train.shape, y_train.shape, X_test.shape, y_test.shape = {self.X_train.shape, self.y_train.shape, self.X_test.shape, self.y_test.shape}')\n",
    "\n",
    "            logging.info(\"Out load_data\")\n",
    "        except Exception as e:\n",
    "            logging.info(e)\n",
    "            raise CustomException(e, sys)\n",
    "\n",
    "    def train_and_evaluate(self):\n",
    "        try:\n",
    "            logging.info(\"In train_and_evaluate\")\n",
    "\n",
    "            # Create a NeuralNetClassifier instance that wraps our PyTorch model.\n",
    "            net = NeuralNetClassifier(\n",
    "            module=ClassifierModule, \n",
    "            criterion=nn.CrossEntropyLoss, \n",
    "            optimizer=torch.optim.Adam, \n",
    "            optimizer__weight_decay=0.01, \n",
    "            max_epochs=10, \n",
    "            lr=0.01, \n",
    "            batch_size=64, \n",
    "            module__input_dim=self.X_train.shape[1], \n",
    "            module__num_hidden_layers=1, \n",
    "            module__hidden_units=50, \n",
    "            module__dropout=0.5, \n",
    "            iterator_train__shuffle=True, \n",
    "            device='cuda' if torch.cuda.is_available() else 'cpu', \n",
    "            verbose=1 \n",
    "        )\n",
    "\n",
    "            # load params from directory\n",
    "            params_path = self.model_trainer_config.PARAMS_FILE_PATH\n",
    "            params = load_json(params_path)\n",
    "            logging.info(f\"params loaded from {{{params_path}}}\")\n",
    "            params[\"optimizer\"] = [torch.optim.SGD, torch.optim.Adam] \n",
    "            logging.info(f\"addition on params ---> params[\\\"optimizer\\\"] = [torch.optim.SGD, torch.optim.Adam]\")\n",
    "\n",
    "            # mlflow logging \n",
    "            with mlflow.start_run():\n",
    "                # grid search object \n",
    "                self.grid = GridSearchCV(net, params, refit=True, cv=3, scoring='accuracy', n_jobs=-1)\n",
    "\n",
    "                # fit on grid\n",
    "                self.grid.fit(self.X_train.astype(np.float32), self.y_train.astype(np.int64))\n",
    "                logging.info(f\"best Score on grid search:{self.grid.best_score_}\")\n",
    "\n",
    "            logging.info(\"Out train_and_evaluate\")\n",
    "        except Exception as e:\n",
    "            logging.info(e)\n",
    "            raise CustomException(e, sys)\n",
    "        \n",
    "    def test(self):\n",
    "        try:\n",
    "            logging.info(\"In test\")\n",
    "\n",
    "            # prediction \n",
    "            predictions = self.grid.best_estimator_.predict(self.X_test.astype(np.float32))\n",
    "\n",
    "            # calculate accuracy \n",
    "            self.test_score_ = accuracy_score(self.y_test.astype(np.int64), predictions)\n",
    "            logging.info(f\"score on test data {{{self.test_score_}}}\")\n",
    "\n",
    "            logging.info(\"Out test\")\n",
    "        except Exception as e:\n",
    "            logging.info(e)\n",
    "            raise CustomException(e, sys)\n",
    "        \n",
    "    def save_outputs(self):\n",
    "        try:\n",
    "            logging.info(\"In save_outputs\")\n",
    "\n",
    "            # save best params\n",
    "            best_params_path = self.model_trainer_config.BEST_PARAMS_FILE_PATH\n",
    "            dump_json(self.grid.best_params_, best_params_path)\n",
    "            logging.info(f\"saved best params at {{{best_params_path}}}\")\n",
    "\n",
    "            # save scoreS\n",
    "            scores_file_path = self.model_trainer_config.SCORES_FILE_PATH\n",
    "            dump_json({\n",
    "                \"grid_best_score_\":self.grid.best_score_,\n",
    "                \"test_score_\":self.test_score_\n",
    "            }, scores_file_path)\n",
    "            logging.info(f\"saved scores at {{{scores_file_path}}}\")\n",
    "\n",
    "            # save model\n",
    "            estimator_path = self.model_trainer_config.ESTIMATOR_FILE_PATH\n",
    "            save_pickle(estimator_path, self.grid.best_estimator_)\n",
    "            logging.info(f\"saved estimator at {{{estimator_path}}}\")\n",
    "\n",
    "            logging.info(\"Out save_outputs\")\n",
    "        except Exception as e:\n",
    "            logging.info(e)\n",
    "            raise CustomException(e, sys)\n",
    "    \n",
    "    def main(self):\n",
    "        # create required directories \n",
    "        create_dirs(self.model_trainer_config.ROOT_DIR_PATH)\n",
    "        create_dirs(self.model_trainer_config.MODEL_ROOT_DIR_PATH)\n",
    "        create_dirs(self.model_trainer_config.TRAINING_ROOT_DIR_PATH)\n",
    "\n",
    "        self.load_data()\n",
    "        mlflow.autolog()\n",
    "        self.train_and_evaluate()\n",
    "        self.test()\n",
    "        self.save_outputs() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c728cbb8",
   "metadata": {},
   "source": [
    "# pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24ee53d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from churn_modelling.configuration import DataTransformationConfig\n",
    "from dataclasses import dataclass \n",
    "\n",
    "\n",
    "@dataclass \n",
    "class ModelTrainerPipeline:\n",
    "    def run(self):\n",
    "        obj = ModelTrainerComponents(DataTransformationConfig, ModelTrainerConfig)\n",
    "        obj.main()\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    data_transformation_pipeline = ModelTrainerPipeline()\n",
    "    data_transformation_pipeline.run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3615ab0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Churn_Modelling",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
